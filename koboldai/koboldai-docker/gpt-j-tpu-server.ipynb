{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "accelerator": "TPU",
        "colab": {
            "name": "GPT-J | KoboldAI Server",
            "provenance": [],
            "collapsed_sections": [],
            "machine_shape": "hm"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "2HflRYUWt8jo"
            },
            "source": [
                "**Welcome to the KoboldAI Colab Service GPT-J-6B Notebook!**<br/>\n",
                "*Note: This colab is intended to be used with the KoboldAI Client, [which can be downloaded from GitHub here](https://github.com/KoboldAI/KoboldAI-Client).*"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "h5NcA61O-S02",
                "cellView": "form"
            },
            "source": [
                "#@title <b>Step 1 - Install Dependencies</b>\n",
                "#@markdown Press the Play button and wait for the script to finish.\n",
                "from IPython.display import clear_output\n",
                "from termcolor import colored\n",
                "import os\n",
                "\n",
                "!pip install flask-ngrok\n",
                "!pip install termcolor\n",
                "!pip install flask_cloudflared\n",
                "!apt install zstd\n",
                "if not os.path.isdir(\"step_383500\"):\n",
                "   !time wget https://the-eye.eu/public/AI/GPT-J-6B/step_383500_slim.tar.zstd\n",
                "   !time tar -I zstd -xf step_383500_slim.tar.zstd\n",
                "!git clone https://github.com/kingoflolz/mesh-transformer-jax.git\n",
                "!pip install -r mesh-transformer-jax/requirements.txt\n",
                "!pip install mesh-transformer-jax/ jax==0.2.12\n",
                "clear_output()\n",
                "print(colored(\"Installing DONE!\", \"green\"))"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "FUpeliUiaUi3",
                "cellView": "form"
            },
            "source": [
                "#@title <b>Step 2 - Adjust Your Settings</b>\n",
                "#@markdown 1. Connect via Ngrok or Cloudflare?\n",
                "connect_method = \"Cloudflare\" #@param [\"Ngrok\", \"Cloudflare\"]\n",
                "#@markdown 2. Press Play button to lock in settings <b>(Do not skip!)</b>"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "8aGzVuDR_eo1"
            },
            "source": [
                "#@title <b>Step 3 - Initialize Model</b> { display-mode: \"form\" }\n",
                "#@markdown Press the Play button. Wait for the model to complete\n",
                "#@markdown initialization. This can take 5+ minutes.</br>\n",
                "#@markdown When the word DONE! is displayed, you can move on to\n",
                "#@markdown the next Step.</br></br>\n",
                "#@markdown <b>>> If you get an error when running this cell, run it again! <<</b>\n",
                "\n",
                "from flask import Flask, redirect, url_for, request\n",
                "import json\n",
                "import torch\n",
                "import requests\n",
                "import subprocess\n",
                "import tarfile\n",
                "from jax.config import config\n",
                "import time\n",
                "\n",
                "# Sometimes the next step errors for some reason, just run it again\n",
                "import jax\n",
                "from jax.experimental import maps\n",
                "import numpy as np\n",
                "import optax\n",
                "import transformers\n",
                "from mesh_transformer.checkpoint import read_ckpt\n",
                "from mesh_transformer.sampling import nucleaus_sample\n",
                "from mesh_transformer.transformer_shard import CausalTransformer\n",
                "\n",
                "# Initialize the model\n",
                "print(colored(\"Initializing model, please wait...\", \"magenta\"))\n",
                "\n",
                "colab_tpu_addr = os.environ['COLAB_TPU_ADDR'].split(':')[0]\n",
                "url = f'http://{colab_tpu_addr}:8475/requestversion/tpu_driver0.1_dev20210607'\n",
                "requests.post(url)\n",
                "config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
                "config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']\n",
                "\n",
                "params = {\n",
                "  \"layers\": 28,\n",
                "  \"d_model\": 4096,\n",
                "  \"n_heads\": 16,\n",
                "  \"n_vocab\": 50400,\n",
                "  \"norm\": \"layernorm\",\n",
                "  \"pe\": \"rotary\",\n",
                "  \"pe_rotary_dims\": 64,\n",
                "\n",
                "  \"seq\": 2048,\n",
                "  \"cores_per_replica\": 8,\n",
                "  \"per_replica_batch\": 1,\n",
                "}\n",
                "\n",
                "per_replica_batch = params[\"per_replica_batch\"]\n",
                "cores_per_replica = params[\"cores_per_replica\"]\n",
                "seq = params[\"seq\"]\n",
                "\n",
                "\n",
                "params[\"sampler\"] = nucleaus_sample\n",
                "params[\"optimizer\"] = optax.scale(0)\n",
                "\n",
                "mesh_shape = (jax.device_count() // cores_per_replica, cores_per_replica)\n",
                "devices = np.array(jax.devices()).reshape(mesh_shape)\n",
                "\n",
                "maps.thread_resources.env = maps.ResourceEnv(maps.Mesh(devices, ('dp', 'mp')))\n",
                "tokenizer = transformers.GPT2TokenizerFast.from_pretrained('gpt2')\n",
                "total_batch = per_replica_batch * jax.device_count() // cores_per_replica\n",
                "print(colored(\"Creating CasualTransformer instance...\", \"magenta\"))\n",
                "network = CausalTransformer(params)\n",
                "print(colored(\"Reading checkpoint...\", \"magenta\"))\n",
                "network.state = read_ckpt(network.state, \"step_383500/\", devices.shape[1])\n",
                "print(colored(\"Calling move_xmap...\", \"magenta\"))\n",
                "network.state = network.move_xmap(network.state, np.zeros(cores_per_replica))\n",
                "\n",
                "clear_output()\n",
                "print(colored(\"DONE!\", \"green\"))"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "spRcqOxM3CqX"
            },
            "source": [
                "#@title <b>Step 4 - Run Web Service</b> { display-mode: \"form\" }\n",
                "#@markdown Press the Play button. Flask will start and give you an \n",
                "#@markdown Ngrok address which looks like this:<br/>\n",
                "#@markdown <i>https://\\<unique id\\>.trycloudflare.com/</i><br/>\n",
                "#@markdown You will need to right-click this and copy the address.\n",
                "#@markdown Start the KoboldAI Client on your computer and choose \n",
                "#@markdown Google Colab as the model. You will be asked to paste \n",
                "#@markdown the Ngrok address into the terminal.<br/><br/>\n",
                "#@markdown If your session is interrupted, you can just restart\n",
                "#@markdown this cell to get a new address without reinitializing\n",
                "#@markdown the model.</br></br>\n",
                "#@markdown <b>The first generation takes around a minute due to \n",
                "#@markdown compilation, but after that it should only take about \n",
                "#@markdown 10 seconds per sample.</b>\n",
                "\n",
                "tenv = maps.ResourceEnv(maps.Mesh(devices, ('dp', 'mp')))\n",
                "\n",
                "if connect_method == \"Cloudflare\":\n",
                "   from flask_cloudflared import run_with_cloudflared\n",
                "elif connect_method == \"Ngrok\":\n",
                "   from flask_ngrok import run_with_ngrok\n",
                "\n",
                "app = Flask(__name__)\n",
                "\n",
                "if connect_method == \"Cloudflare\":\n",
                "   run_with_cloudflared(app)\n",
                "elif connect_method == \"Ngrok\":\n",
                "   run_with_ngrok(app)\n",
                "\n",
                "@app.route(\"/\")\n",
                "def home():\n",
                "    return \"<h1>KoboldAI Colab Service Running!</h1>\"\n",
                "\n",
                "@app.route('/request',methods = ['POST'])\n",
                "def koboldrequest():\n",
                "   if request.method == 'POST':\n",
                "      try:\n",
                "        clear_output()\n",
                "        js      = request.json\n",
                "        txt     = js[\"text\"]\n",
                "        min     = js[\"min\"]\n",
                "        max     = js[\"max\"]\n",
                "        rep_pen = js[\"rep_pen\"]\n",
                "        temp    = js[\"temperature\"]\n",
                "        top_p   = js[\"top_p\"]\n",
                "\n",
                "        gen_len = max - (min - 1)\n",
                "\n",
                "        print(colored(\"Received Data: {0}\".format(txt), \"yellow\"))\n",
                "        print(colored(\"Generating text, please wait...\", \"green\"))\n",
                "\n",
                "        # env has to be redefined with each call for some reason, else a threading error is produced\n",
                "        maps.thread_resources.env = tenv\n",
                "        \n",
                "        tokens = tokenizer.encode(txt)\n",
                "        provided_ctx = len(tokens)\n",
                "        pad_amount = seq - provided_ctx\n",
                "        padded_tokens = np.pad(tokens, ((pad_amount, 0),)).astype(np.uint32)\n",
                "        batched_tokens = np.array([padded_tokens] * total_batch)\n",
                "        length = np.ones(total_batch, dtype=np.uint32) * len(tokens)\n",
                "        output = network.generate(batched_tokens, length, gen_len, {\"top_p\": np.ones(total_batch) * top_p, \"temp\": np.ones(total_batch) * temp})\n",
                "        samples = []\n",
                "        decoded_tokens = output[1][0]\n",
                "\n",
                "        for o in decoded_tokens[:, :, 0]:\n",
                "          samples.append(tokenizer.decode(o))\n",
                "\n",
                "        genout = samples[0]\n",
                "\n",
                "        print(colored(\"Generated Text: {0}\".format(genout), \"cyan\"))\n",
                "        response = app.response_class(\n",
                "           response=json.dumps({\"data\": {\"seqs\": [genout]}}),\n",
                "           status=200,\n",
                "           mimetype='application/json'\n",
                "        )\n",
                "        \n",
                "        js         = {}\n",
                "        genout     = \"\"\n",
                "        \n",
                "        return response\n",
                "\n",
                "      except Exception as e:\n",
                "        print(colored(\"[ERROR] Something went wrong during generation!\", \"red\"))\n",
                "        print(colored(\"{0}\".format(e), \"red\"))\n",
                "        response = app.response_class(\n",
                "          response=json.dumps({\"error\": {\"extensions\": {\"code\": \"Something went wrong during generation! {0}\".format(e)}}}),\n",
                "          status=400,\n",
                "          mimetype='application/json'\n",
                "        )\n",
                "\n",
                "print(colored(\"Starup complete! Running web service.\", \"green\"))\n",
                "app.run()"
            ],
            "execution_count": null,
            "outputs": []
        }
    ]
}
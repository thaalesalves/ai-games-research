{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-Neo | Finetuning a Model",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp299JvbFoyU"
      },
      "source": [
        "# Finetuning a model for NovelAI\n",
        "Welcome to javaman's Colab Notebook for training your own GPT-Neo model for NovelAI. Follow these steps and you'll be playing NAI with your model in no time. Do join [NovelAI Discord](https://discord.gg/q9XU7fchbY) if you're not a member already, and help us build it!\n",
        "\n",
        "## Requirements\n",
        "* GPT-Neo\n",
        "* Google Cloud Storage (a bucket)\n",
        "* A Colab Network with TPUs available (if you have Colab Pro, even better)\n",
        "* Conversion from TensorFlow model to a Huggingface model\n",
        "* NovelAI Colab (mine or finetune's, either works)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mqN-smtJPTA"
      },
      "source": [
        "## Getting started"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwSYAosEGoAD",
        "cellView": "form"
      },
      "source": [
        "#@title Set up GPT-Neo\n",
        "#@markdown This step will set up needed stuff for training your model. It will download GPT-Neo and install other dependencies.\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "!git clone https://github.com/EleutherAI/GPTNeo\n",
        "%cd GPTNeo\n",
        "!pip3 install -q -r requirements.txt\n",
        "pretrained_model = None\n",
        "dataset = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRbGDj9YHp76",
        "cellView": "form"
      },
      "source": [
        "#@title Set up Google Cloud Storage\n",
        "#@markdown You will need to create a bucket on GCP to use this colab. Once you've created your bucket and given a unique name, you can come here and authenticate on GCP. Use [this link](https://console.cloud.google.com/storage) to create a GCP bucket.\n",
        "\n",
        "cloud_bucket = 'gs://your-bucket' #@param {type:\"string\"}\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "!gcloud init"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "0FjkJwYxKLqf"
      },
      "source": [
        "#@title Set up Google Drive\n",
        "#@markdown You might want to store your datasets on GDrive and then use them here. For this, you'll need to authenticate on GDrive.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWNSOh27JUEu",
        "cellView": "form"
      },
      "source": [
        "#@title Set up training dataset\n",
        "#@markdown This cell was imported from the official GPT-Neo Colab Notebook, and gives you options of finetuning models with any dataset you want, including stuff from The Pile. If you wish to use The Pile, just select an option from the dropdown menu.\n",
        "\n",
        "#@markdown * Sampling Only - choose this option if you only wish to sample from our trained models, then move on to the Pretrained Model section.\n",
        "\n",
        "#@markdown * Literotica - erotic texts\n",
        "\n",
        "#@markdown * On penWebText - an opensource clone of OpenAI's WebText dataset, the original training data of GPT2.\n",
        "\n",
        "#@markdown * YoutubeSubtitles - a dataset of subtitles scraped from youtube videos.\n",
        "\n",
        "#@markdown * Hackernews - comments scraped from hackernews\n",
        "\n",
        "#@markdown * Books - collection of novels\n",
        "\n",
        "#@markdown * NIHExporter - Data relating to various projects from the national institute of health.\n",
        "\n",
        "#@markdown * Custom - if this option is chosen you will be prompted to enter the path to your own dataset. It should be a directory containing .txt or .jsonl files.\n",
        "\n",
        "#@markdown All these datasets are from EleutherAI's side project - The Pileâ„¢ - an effort to gather a general purpose, diverse and open source plain text dataset large enough to train 1T+ parameter language models. Alternatively, you can provide your own dataset in the form of a folder or gzip archive of .txt files. Simply select 'Custom' below and follow input the path to your data and the name of your dataset when prompted.\n",
        "\n",
        "#@markdown **Note:** `dataset_name` is an arbitrary name you set. It will be used again when finetuning starts. It doesn't need to match the actual dataset file names.\n",
        "\n",
        "import os\n",
        "dataset = 'Custom' #@param [\"Sampling_Only\", \"Literotica\", \"OpenWebText\", \"YoutubeSubtitles\", \"HackerNews\", \"Books\", \"NIHExporter\", \"Custom\"]\n",
        "\n",
        "if dataset == \"Sampling_Only\":\n",
        "  pass\n",
        "elif dataset == 'OpenWebText':\n",
        "  !wget https://the-eye.eu/public/AI/pile_preliminary_components/openwebtext2.jsonl.zst.tar -O openwebtext.tar.xz\n",
        "  !tar xf openwebtext.tar.xz\n",
        "  dataset_path = \"openwebtext\"\n",
        "  dataset_name = dataset_path\n",
        "  out_name = dataset_name + \"_tokenized\"\n",
        "elif dataset == 'Literotica':\n",
        "  os.makedirs('data', exist_ok=True)\n",
        "  !wget https://the-eye.eu/public/AI/pile_preliminary_components/Literotica.jsonl.zst -O data/Literotica.jsonl.zst\n",
        "  dataset_path = 'data'\n",
        "  dataset_name = 'literotica'\n",
        "  out_name = dataset_name + \"_tokenized\"\n",
        "elif dataset == 'YoutubeSubtitles':\n",
        "  os.makedirs('data', exist_ok=True)\n",
        "  !wget https://the-eye.eu/public/AI/pile_preliminary_components/yt_subs.jsonl.zst -O data/yt_subs.jsonl.zst\n",
        "  dataset_path = 'data'\n",
        "  dataset_name = 'ytsubs'\n",
        "  out_name = dataset_name + \"_tokenized\"\n",
        "elif dataset == 'HackerNews':\n",
        "  os.makedirs('data', exist_ok=True)\n",
        "  !wget https://the-eye.eu/public/AI/pile_preliminary_components/hn.tar.gz -O data/hn.tar.gz\n",
        "  dataset_path = 'data'\n",
        "  dataset_name = 'hackernews'\n",
        "  out_name = dataset_name + \"_tokenized\"\n",
        "elif dataset == 'Books':\n",
        "  os.makedirs('data', exist_ok=True)\n",
        "  !wget https://the-eye.eu/public/AI/pile_preliminary_components/books3.tar.gz -O data/Books.tar.gz\n",
        "  dataset_path = 'data'\n",
        "  dataset_name = 'books'\n",
        "  out_name = dataset_name + \"_tokenized\"\n",
        "elif dataset == \"NIHExporter\":\n",
        "  os.makedirs('data', exist_ok=True)\n",
        "  !wget https://the-eye.eu/public/AI/pile_preliminary_components/NIH_ExPORTER_awarded_grant_text.jsonl.zst -O data/NIH_ExPORTER_awarded_grant_text.jsonl.zst\n",
        "  dataset_path = 'data'\n",
        "  os.system('mv NIH_ExPORTER_awarded_grant_text.jsonl.zst ./data')\n",
        "  dataset_name = 'nihexporter'\n",
        "  out_name = dataset_name + \"_tokenized\"\n",
        "elif dataset == \"Custom\":\n",
        "  dataset_path = input('Enter the path to the folder containing your data: ')\n",
        "  dataset_name = input('Enter the name of your dataset: ')\n",
        "  out_name = dataset_name + \"_tokenized\"\n",
        "else:\n",
        "  raise NotImplementedError('please select from available options: [\"OpenWebText\", \"YoutubeSubtitles\", \"HackerNews\", \"NIHExporter\", \"Custom\"]')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTXnDD75Lxiy",
        "cellView": "form"
      },
      "source": [
        "#@title Tokenize your dataset\n",
        "#@markdown This step will read through the dataset your provided and tokenize its contents. The tokens will then be stored on your GCP bucket.\n",
        "\n",
        "!python data/create_tfrecords.py --input_dir $dataset_path --name $dataset_name --files_per 1000 --output_dir $out_name --write_dataset_config --processes 1\n",
        "\n",
        "print(\"Your GCP bucket path: \" + cloud_bucket)\n",
        "print(\"Dataset type: \" + dataset)\n",
        "print(\"Dataset name: \" + dataset_name)\n",
        "print(\"Dataset path: \" + dataset_path)\n",
        "\n",
        "if not cloud_bucket.endswith('/'):\n",
        "  cloud_bucket += '/'\n",
        "\n",
        "copy_loc = cloud_bucket + \"datasets/\" + dataset\n",
        "!gsutil -m cp -r /content/GPTNeo/$out_name $copy_loc\n",
        "!gsutil ls $cloud_bucket"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oH3RNLeKZXd"
      },
      "source": [
        "## Finetuning the model\n",
        "Now that we've set up everything, we can begin finetuning our model. You can either finetune a model from scratch, or keep your tuning from a checkpoint from an existent model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Afvuxf8EM9ts"
      },
      "source": [
        "#### Configuration files\n",
        "Now you will need to write some configurations for your model. Do keep in mind that you'll need to alter some infos on the files below before being able to use them. The relative paths here all point to /content/GPTNeo.\n",
        "\n",
        "#### First file (dataset config)\n",
        "* Change _**YOUR-MODEL-NAME.json**_ to, well... your model's name so the file gets saved.\n",
        "* Change _**gs://your-unique-bucket-name**_ to... you know what to do.\n",
        "* Change the path to the one that matches where your _tfrecords_ files are. Do notice the wildcard there. Leave it, just change what's before it.\n",
        "\n",
        "#### Second file (model config)\n",
        "* _**YOUR-MODEL-NAME.json**_... you know what to do.\n",
        "* The prop _**model_path**_ needs to be changed as well. It needs to be pointed to your bucket, to a folder that doens't exist yet. This folder will be created and your finetuned checkpoints will be stored there.\n",
        "* The prop _**datasets**_ needs to be modified to the dataset config created before. That is, the name of the first file (or, if you set up various datasets, add them to the array too).\n",
        "* The prop _**iterations**_ refers to how many steps the finetuning will go before it's saved. We recommend a value higher than 500, and if you're doing 2K or more steps, try increasing it even more. The more often the checkpoints are saved, the heavier the model will be, so try to keep it to a minimum (which means a higher value, but never higher than the quantity of steps you set for the training).\n",
        "\n",
        "**Note:** this model config file refers to the 1.5B model. To update it with your own or use the 2.7B config, just open the model folder and paste the contents of the file `config.json` in the second cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn1pBJVZM7P8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94dfd25e-4454-4d70-a923-a49317afa40a"
      },
      "source": [
        "%%writefile configs/dataset_configs/your_dataset_name.json\n",
        "\n",
        "{\n",
        "  \"path\": \"gs://amaranth-ai/datasets/Custom/your_dataset_name_tokenized/your_dataset_name*.tfrecords\",\n",
        "  \"eval_path\": \"\",\n",
        "  \"n_vocab\": 50256,\n",
        "  \"tokenizer_is_pretrained\": true,\n",
        "  \"tokenizer_path\": \"gpt2\",\n",
        "  \"eos_id\": 50256,\n",
        "  \"padding_id\": 50257\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing configs/dataset_configs/teslore_complete.txt.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjdbwmoSZRaS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ffb675e-53ab-4dd4-d2ec-b0126b22944c"
      },
      "source": [
        "%%writefile configs/custom_model.json\n",
        "\n",
        "{\n",
        "  \"n_head\": 20,\n",
        "  \"n_vocab\": 50257,\n",
        "  \"embed_dropout\": 0,\n",
        "  \"lr\": 0.00016,\n",
        "  \"lr_decay\": \"cosine\",\n",
        "  \"warmup_steps\": 3000,\n",
        "  \"beta1\": 0.9,\n",
        "  \"beta2\": 0.95,\n",
        "  \"epsilon\": 1e-08,\n",
        "  \"ada_epsilon1\": \"1e-30\",\n",
        "  \"ada_epsilon2\": 0.001,\n",
        "  \"opt_name\": \"adam\",\n",
        "  \"weight_decay\": 0,\n",
        "  \"train_batch_size\": 512,\n",
        "  \"attn_dropout\": 0,\n",
        "  \"train_steps\": 400000,\n",
        "  \"lr_decay_end\": 300000,\n",
        "  \"eval_steps\": 10,\n",
        "  \"predict_steps\": 0,\n",
        "  \"res_dropout\": 0,\n",
        "  \"eval_batch_size\": 128,\n",
        "  \"predict_batch_size\": 1,\n",
        "  \"iterations\": 1000,\n",
        "  \"n_embd\": 2560,\n",
        "  \"datasets\": [\n",
        "    [\"your_dataset_name\", null, null, null]\n",
        "  ],\n",
        "  \"model_path\": \"gs://neo-d/models/GPT3_2-7B\",\n",
        "  \"n_ctx\": 2048,\n",
        "  \"n_layer\": 32,\n",
        "  \"scale_by_depth\": true,\n",
        "  \"scale_by_in\": false,\n",
        "  \"attention_types\": [\n",
        "    [\n",
        "      [\"global\", \"local\"], 16]\n",
        "  ],\n",
        "  \"mesh_shape\": \"x:64,y:4\",\n",
        "  \"layout\": \"batch:x,embd:y\",\n",
        "  \"activation_function\": \"gelu\",\n",
        "  \"recompute_grad\": true,\n",
        "  \"gradient_clipping\": 1.0,\n",
        "  \"tokens_per_mb_per_replica\": 4096,\n",
        "  \"padding_id\": 50257,\n",
        "  \"eos_id\": 50256\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing configs/custom_model.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3oXL0nkQsvO"
      },
      "source": [
        "#### Setting things up\n",
        "EleutherAI has two pre-trained models, one with [1.3B parameters](https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/), and another with [2.7B](https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/). We will use those to finetune with our own dataset. Be advised that the 2.7B model is really heavy, and you won't be able to use Colab to train a model that big with a batch size greater than 2, because the Colab VM will start throwing memory errors. If you want to do heavy-duty training of the 2.7B model, consider [renting instances made for this kind of work](https://vast.ai/console/create/).\n",
        "\n",
        "* `pretrained_model`: the name of the model you're going to finetune (use the options in the dropdown for vanilla models)\n",
        "* `is_vanilla_model`: check only if you're going to finetune 1.5B or 2.7B in their original state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqei66sbuoHp",
        "cellView": "form"
      },
      "source": [
        "#@markdown #### Set up the model\n",
        "pretrained_model = \"GPT3_XL\" #@param [\"GPT3_XL\", \"GPT3_2-7B\"] {allow-input: true}\n",
        "is_vanilla_model = True #@param {type:\"boolean\"}\n",
        "\n",
        "if is_vanilla_model:\n",
        "  !wget --cut-dirs=4 -nH -r -m -np -c -U \"eye02\" -w 2 -R \"index.html*\" \"https://the-eye.eu/public/AI/gptneo-release/$pretrained_model/\" -P \"/content/models/$pretrained_model/\"\n",
        "\n",
        "path_to_local_weights = f\"/content/models/{pretrained_model}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_qDSAYSu1V4",
        "cellView": "form"
      },
      "source": [
        "#@markdown #### Copy the downloaded model to your bucket\n",
        "#@markdown * `delete_from_colab`: if checked, deletes the downloaded model from local storage in Colab.\n",
        "\n",
        "delete_from_colab = True #@param{type:'boolean'}\n",
        "\n",
        "# Copy the downloaded model to your bucket\n",
        "bucket_base = \"gs://\" + cloud_bucket.replace('gs://', '').split('/')[0]\n",
        "!gsutil -m cp -r $path_to_local_weights $bucket_base\n",
        "\n",
        "if delete_from_colab:\n",
        "  !rm -rf $path_to_local_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9E5-nvRvF3-"
      },
      "source": [
        "#### Begin finetuning\n",
        "Now we want to make a few modifications to the model config in order to get training / sampling working on colab.\n",
        "\n",
        "If you are just sampling from our pretrained models, you can leave the settings as is, run the cell below, then move on to the `Sample from your model` section.\n",
        "\n",
        "If finetuning, you can change parameters below. \n",
        "\n",
        "* `path_to_model`: where your model files are located in your bucket\n",
        "\n",
        "* `batch_size`: is your train batch size - if you're encountering memory errors, try lowering this. (**Note:** Colab can only handle up to 2 with the 2.7B model)\n",
        "\n",
        "* `mesh_shape`: specifies the way the model will be divided up across the TPU cores. We suggest leaving this alone unless you know what you're doing.\n",
        "\n",
        "* `train_steps`: specifies how many steps you want the model to finetune for. If you are just sampling from the model, you can leave this as is.\n",
        "\n",
        "* `steps_per_checkpoint`: specifies how often you want to save model weights during training.\n",
        "\n",
        "* `start_step`: the checkpoint which your training will continue from. Check the latest checkpoint your model has and continue from it. If you're using the vanilla 2.7B, set it to 400000, or 362000 for the 1.5B.\n",
        "\n",
        "* `config_filename`: config file name. It should be the one you created before this step. It will be located in `/content/GPTNeo/configs`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGKRyjzXvI4B",
        "cellView": "form"
      },
      "source": [
        "import json\n",
        "from pprint import pprint\n",
        "path_to_model = \"\" #@param {type:\"string\"}\n",
        "batch_size =  2#@param {type:\"integer\"}\n",
        "mesh_shape = \"x:4,y:2\" #@param {type:\"string\"}\n",
        "train_steps = 15000 #@param {type:\"integer\"}\n",
        "steps_per_checkpoint = 1000 #@param {type:\"integer\"}\n",
        "start_step = 415000 #@param {type:\"integer\"}\n",
        "config_filename = 'custom_model.json' #@param {type:\"string\"}\n",
        " \n",
        "if path_to_model == \"\":\n",
        "  path_to_model = \"gs://\" + cloud_bucket.replace('gs://', '').replace('/', '') + f'/{pretrained_model}'\n",
        "print(f'MODEL PATH: {path_to_model}\\n')\n",
        "\n",
        "if dataset_name == \"\" and dataset != \"Sampling_Only\":\n",
        "  dataset_name = dataset\n",
        "elif dataset is None and dataset_name == \"\":\n",
        "  dataset_name = \"pile\"\n",
        " \n",
        "def pad_to_multiple_of(n, mult):\n",
        "  \"\"\"\n",
        "  pads n to a multiple of mult\n",
        "  \"\"\"\n",
        "  extra = n % mult\n",
        "  if extra > 0:\n",
        "    n = n + mult - extra\n",
        "  return n\n",
        "\n",
        "with open(f'/content/GPTNeo/configs/{config_filename}', 'r') as f:\n",
        "  data = json.load(f)\n",
        "  pprint(data)\n",
        "  dset_val = [[dataset_name, None, None, None]] if dataset_name != \"\" else data[\"datasets\"]\n",
        "  mods = {\n",
        "          \"mesh_shape\": mesh_shape,\n",
        "          \"layout\": \"intermediate_expanded:x,heads:x,memory_length:y,embd:y\",\n",
        "          \"model_path\": path_to_model,\n",
        "          \"datasets\": dset_val,\n",
        "          \"train_steps\": start_step + train_steps,\n",
        "          \"eval_steps\": 0,\n",
        "          \"train_batch_size\": batch_size,\n",
        "          \"predict_batch_size\": batch_size\n",
        "        }\n",
        "  data.update(mods)\n",
        "  print('\\n--->\\n')\n",
        "  pprint(data)\n",
        "  with open(f'configs/{pretrained_model}.json', 'w') as outfile:\n",
        "    json.dump(data, outfile, indent=2)\n",
        "\n",
        "!python3 main.py --model $pretrained_model --steps_per_checkpoint $steps_per_checkpoint --tpu colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ23jF-SPpML"
      },
      "source": [
        "## Sampling from your model\n",
        "Now we can try sampling stuff from our model. First, let's create an initial prompt for the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4sd2VkuUT9p",
        "cellView": "form"
      },
      "source": [
        "#@markdown After saving the sample file, let's try generating an output from the model. Type a prompt and then run this cell to test it.\n",
        "!rm -rf example_prompt.txt\n",
        "prompt = \"You are a knight in the kingdom of Larion\" #@param{type:'string'}\n",
        "\n",
        "!echo -n $prompt > example_prompt.txt\n",
        "!python3 main.py --model $pretrained_model --steps_per_checkpoint $steps_per_checkpoint --tpu colab --predict --prompt example_prompt.txt"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GPT-2 | KoboldAI Server",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyfkCfyvMeLk"
      },
      "source": [
        "# Playing with KoboldAI\n",
        "This notebook lets you sample from your model, as well as opening an endpoint that connects with KoboldAI client for you to play your storytelling games. Just follow the instructions from [KoboldAI's GitHub page](https://github.com/KoboldAI/KoboldAI-Client) and also the instructions here, and voil√†. To use KoboldAI with the default models, access their [official Colab notebook](https://colab.research.google.com/drive/1uGe9f4ruIQog3RLxfUsoThakvLpHjIkX?usp=sharing#scrollTo=h5NcA61O-S02)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVnEi4ooMrS7",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86ae73f0-e0d6-43b1-e6b0-c6e351efb595"
      },
      "source": [
        "#@title <b>Install Dependencies</b>\n",
        "#@markdown Press the Play button and wait for the script to finish.\n",
        "from IPython.display import clear_output\n",
        "from termcolor import colored\n",
        " \n",
        "!pip install flask-ngrok\n",
        "!pip install git+https://github.com/finetuneanon/transformers@gpt-neo-dungeon-localattention2\n",
        "!pip install termcolor\n",
        "!pip install flask_cloudflared\n",
        "clear_output()\n",
        "print(colored(\"DONE!\", \"green\"))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32mDONE!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdxEVU4ia2Ov",
        "cellView": "form"
      },
      "source": [
        "#@title <b>Set up Google Drive</b>\n",
        "#@markdown Run this cell to mount your Google Drive folder in the colab.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "U_CNkDBfd02L"
      },
      "source": [
        "#@title <b>Check memory and GPU</b>\n",
        "#@markdown Run this cell to print VRAM usage and allocation, as well as which GPU you're currently using.\n",
        "\n",
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "print('\\n\\n====GPU INFO====')\n",
        "!nvidia-smi\n",
        "\n",
        "print('\\n\\n====CUDA MEMORY STATS====')\n",
        "cuda_memory = torch.cuda.memory_summary(device=None, abbreviated=False)\n",
        "print(cuda_memory.replace('\\\\n', '\\n'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "E-w2wf_yiHEY",
        "outputId": "b4e4f802-5d7d-41b1-cdfd-97bf27b38ae5"
      },
      "source": [
        "#@title <b>Run service</b>\n",
        "model_name = \"microsoft/DialoGPT-large\" #@param [\"microsoft/DialoGPT-large\", \"luca-martial/DialoGPT-Elon\", \"gpt2-large\"]  {allow-input: true}\n",
        "is_archived = False #@param {type:'boolean'}\n",
        "connect_method = \"Ngrok\" #@param [\"Ngrok\", \"Cloudflare\"]\n",
        "precision = \"Full\" #@param [\"Half\", \"Full\"]\n",
        "\n",
        "#@markdown This notebook will extract the model tar file (if necessary) \n",
        "#@markdown and initialize it. <b>This will take several minutes.</b>\n",
        "#@markdown When the model is ready, Flask will start and give you a \n",
        "#@markdown Cloudflare or Ngrok address which looks like this:<br/>\n",
        "#@markdown <i>https://\\<unique id\\>.trycloudflare.com/</i><br/>\n",
        "#@markdown <i>http://\\<unique id\\>.ngrok.io/</i><br/>\n",
        "#@markdown <br/>\n",
        "#@markdown You will need to right-click this and copy the address.\n",
        "#@markdown Start the KoboldAI Client on your computer and choose \n",
        "#@markdown Google Colab as the model. You will be asked to paste \n",
        "#@markdown the copied address into the terminal.\n",
        "\n",
        "from flask import Flask, redirect, url_for, request\n",
        "import json\n",
        "import torch\n",
        "import requests\n",
        "import subprocess\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline, AutoTokenizer\n",
        "import tarfile\n",
        "from google.colab import drive\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import gc\n",
        "from threading import Timer\n",
        "import numpy as np\n",
        "\n",
        "if connect_method == \"Cloudflare\":\n",
        "   from flask_cloudflared import run_with_cloudflared\n",
        "elif connect_method == \"Ngrok\":\n",
        "   from flask_ngrok import run_with_ngrok\n",
        "\n",
        "# Thanks to finetune for some of this startup code, I'm really not\n",
        "# familiar with the Colab environment\n",
        "model         = None\n",
        "tokenizer     = None\n",
        "custom_models = [\"2.7B-horni\", \"2.7B-horni-ln\", \"amaranth-2.7B\"]\n",
        "\n",
        "# Get access to the unpacked model folder\n",
        "if model_name in custom_models:\n",
        "   if is_archived:\n",
        "      # Archived. Set path to tar file and unpack it\n",
        "      model_gdrive = \"/content/drive/MyDrive/gpt-neo-{0}.tar\".format(model_name)\n",
        "      if not os.path.isdir(\"gpt-neo-\"+model_name):\n",
        "         print(colored(\"Unpacking tar file, please wait...\", \"magenta\"))\n",
        "         tar = tarfile.open(model_gdrive, \"r\")\n",
        "         tar.extractall()\n",
        "         tar.close()\n",
        "         print(colored(\"DONE!\", \"green\"))\n",
        "   else:\n",
        "      # Unpacked model already available, just set the path to it\n",
        "      model_gdrive = \"/content/drive/MyDrive/gpt-neo-{0}\".format(model_name)\n",
        "\n",
        "# Initialize the model\n",
        "print(colored(\"Initializing model, please wait...\", \"magenta\"))\n",
        "\n",
        "if model_name in custom_models:\n",
        "   if is_archived:\n",
        "      # If the model was archived, it now lives in the Colab's /content directory\n",
        "      checkpoint = torch.load(\"gpt-neo-\" + model_name + \"/pytorch_model.bin\", map_location=\"cuda:0\")\n",
        "      model = GPT2LMHeadModel.from_pretrained(\"gpt-neo-\" + model_name, state_dict=checkpoint).half().to(\"cuda\").eval()\n",
        "   else:\n",
        "      # The unpacked folder lives on the user's GDrive\n",
        "      checkpoint = torch.load(model_gdrive + \"/pytorch_model.bin\", map_location=\"cuda:0\")\n",
        "      model = GPT2LMHeadModel.from_pretrained(model_gdrive, state_dict=checkpoint).half().to(\"cuda\").eval()\n",
        "else:\n",
        "   from transformers.file_utils import cached_path, WEIGHTS_NAME, hf_bucket_url\n",
        "   archive_file = hf_bucket_url(model_name, filename=WEIGHTS_NAME)\n",
        "   resolved_archive_file = cached_path(archive_file)\n",
        "   checkpoint = torch.load(resolved_archive_file, map_location=\"cuda:0\")\n",
        "   for k in checkpoint.keys():\n",
        "      checkpoint[k] = checkpoint[k].half()\n",
        "   model = GPT2LMHeadModel.from_pretrained(model_name, state_dict=checkpoint).half().to(\"cuda\").eval()\n",
        "\n",
        "for k in list(checkpoint.keys()):\n",
        "   del checkpoint[k]\n",
        "del checkpoint\n",
        "\n",
        "# Initialize the tokenizer and set up the bad_words_ids to exclude Author's Note tags\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "vocab         = tokenizer.get_vocab()\n",
        "vocab_keys    = vocab.keys()\n",
        "find_keys     = lambda char : [key for key in vocab_keys if key.find(char) != -1]\n",
        "bad_words     = []\n",
        "bad_words_ids = []\n",
        "\n",
        "bad_words.extend(find_keys(\"[\"))\n",
        "bad_words.extend(find_keys(\" [\"))\n",
        "for key in bad_words:\n",
        "  bad_id = vocab[key]\n",
        "  bad_words_ids.append([bad_id])\n",
        "\n",
        "# Enable 32-bit mode if the GPU can handle it\n",
        "if precision == \"Full\":\n",
        "  if torch.cuda.get_device_properties(0).total_memory > 15000 * 1024 * 1024:\n",
        "    print(colored(\"Big GPU detected, using fp32\", \"magenta\"))\n",
        "    model = model.float()\n",
        "\n",
        "print(colored(\"DONE!\", \"green\"))\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "if connect_method == \"Cloudflare\":\n",
        "   run_with_cloudflared(app)\n",
        "elif connect_method == \"Ngrok\":\n",
        "   run_with_ngrok(app)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return \"<h1>KoboldAI Colab Service Running!</h1>\"\n",
        "\n",
        "@app.route('/request',methods = ['POST'])\n",
        "def koboldrequest():\n",
        "   if request.method == 'POST':\n",
        "      try:\n",
        "        clear_output()\n",
        "        js      = request.json\n",
        "        txt     = js[\"text\"]\n",
        "        min     = js[\"min\"]\n",
        "        max     = js[\"max\"]\n",
        "        rep_pen = js[\"rep_pen\"]\n",
        "        temp    = js[\"temperature\"]\n",
        "        top_p   = js[\"top_p\"]\n",
        "\n",
        "        # Compatability with un-updated clients\n",
        "        if(\"numseqs\" in js):\n",
        "          numseqs = js[\"numseqs\"]\n",
        "        else:\n",
        "          numseqs = 1\n",
        "\n",
        "        if(\"retfultxt\" in js):\n",
        "          retfultxt = js[\"retfultxt\"]\n",
        "        else:\n",
        "          retfultxt = True\n",
        "\n",
        "        print(colored(\"Received Data: {0}\".format(txt), \"yellow\"))\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        print(colored(\"Generating text, please wait...\", \"green\"))\n",
        "\n",
        "        tokens = tokenizer(txt, return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
        "        ids = tokens.cuda()\n",
        "                                                \n",
        "        def generate(input_str, length=250, n=5):\n",
        "          cur_ids = torch.tensor(tokenizer.encode(input_str)).unsqueeze(0).long().to(\"cuda\")\n",
        "          model.eval()\n",
        "          with torch.no_grad():\n",
        "            for i in range(length):\n",
        "              outputs = model(cur_ids[:, -1024:], labels=cur_ids[:, -1024:])\n",
        "              loss, logits = outputs[:2]\n",
        "              softmax_logits = torch.softmax(logits[0,-1], dim=0)\n",
        "              next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n)\n",
        "              cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(\"cuda\") * next_token_id], dim=1)\n",
        "            output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
        "            output_text = tokenizer.decode(output_list)\n",
        "            return output_text\n",
        "\n",
        "        def choose_from_top(probs, n=5):\n",
        "            ind = np.argpartition(probs, -n)[-n:]\n",
        "            top_prob = probs[ind]\n",
        "            top_prob = top_prob / np.sum(top_prob) # Normalize\n",
        "            choice = np.random.choice(n, 1, p = top_prob)\n",
        "            token_id = ind[choice][0]\n",
        "            return int(token_id)\n",
        "\n",
        "        genout = generate(txt, max).split(\"<|endoftext|>\")[1]\n",
        "        if(len(genout) > 0 and genout != \"\"):\n",
        "          if(retfultxt):\n",
        "            # Outdated client, send old JSON format\n",
        "            print(colored(\"Generated Text: {0}\".format(genout), \"cyan\"))\n",
        "            response = app.response_class(\n",
        "              response=json.dumps({\"data\": {\"text\": genout}}),\n",
        "              status=200,\n",
        "              mimetype='application/json'\n",
        "            )\n",
        "          else:\n",
        "            # New client format with numseq support\n",
        "            i = 0\n",
        "            for seq in genout:\n",
        "              print(colored(\"[Result {0}]\\n{1}\".format(i, seq), \"cyan\"))\n",
        "              i += 1\n",
        "            response = app.response_class(\n",
        "              response=json.dumps({\"data\": {\"seqs\": genout}}),\n",
        "              status=200,\n",
        "              mimetype='application/json'\n",
        "            )\n",
        "\n",
        "          return response\n",
        "        else:\n",
        "          print(colored(\"[ERROR] Something went wrong during generation!\", \"red\"))\n",
        "          response = app.response_class(\n",
        "            response=json.dumps({\"error\": {\"extensions\": {\"code\": \"Something went wrong during generation!\"}}}),\n",
        "            status=400,\n",
        "            mimetype='application/json'\n",
        "          )\n",
        "        \n",
        "        js         = {}\n",
        "        tokens     = []\n",
        "        ids        = []\n",
        "        gen_tokens = []\n",
        "        genout     = \"\"\n",
        "        response   = {}\n",
        "\n",
        "      except Exception as e:\n",
        "        print(colored(\"[ERROR] Something went wrong during generation!\", \"red\"))\n",
        "        print(colored(\"{0}\".format(e), \"red\"))\n",
        "        response = app.response_class(\n",
        "          response=json.dumps({\"error\": {\"extensions\": {\"code\": \"Something went wrong during generation!\"}}}),\n",
        "          status=400,\n",
        "          mimetype='application/json'\n",
        "        )\n",
        "\n",
        "print(colored(\"Starup complete! Running web service.\", \"green\"))\n",
        "app.run()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mReceived Data:  What's on TV?\u001b[0m\n",
            "\u001b[32mGenerating text, please wait...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [04/Jul/2021 05:05:24] \"\u001b[37mPOST /request HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[36mGenerated Text: I think you mean football and the football game\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
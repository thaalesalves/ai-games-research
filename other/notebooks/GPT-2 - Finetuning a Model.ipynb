{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-2 | Finetuning",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyZTTpRScdH0"
      },
      "source": [
        "# **Treinamento de um modelo GPT-2**\n",
        "Este colab contém recursos necessários para criar e treinar um modelo de aprendizado de máquina usando a tecnologia GPT-2 do OpenAI. Este collab faz uso do `gpt-2-simple`, um fork do GPT-2 que torna as coisas mais simples de serem usadas, e não requer uso da API oficial da OpenAI. Você pode ver o código do gpt-2-simple no [GitHub](https://github.com/minimaxir/gpt-2-simple).\n",
        "\n",
        "Este colab é feito baseado no criado pelo [Max Woolf](https://minimaxir.com/), e é uma versão simplificada e em português dele. Primariamente, este colab está sendo usado para treinamento da base de dados do [Grand Prognosticator](https://github.com/thaalesalves/grand-prognosticator), um robô para consulta de lore de Elder Scrolls. \n",
        "\n",
        "Max Woolf escreveu um artigo sobre como criar aplicações que utilizam o modelo de dados treinado pelo GPT-2, e você pode ler mais [aqui](https://minimaxir.com/2019/09/howto-gpt2/).\n",
        "\n",
        "Infelizmente, por conta dos recursos que o Google fornece, é apenas possível treinar o modelo _small_ (124M) e o modelo _medium_ (355M). Os demais modelos podem ser treinados em infraestruturas mais potentes caso você tenha os recursos para bancá-las.\n",
        "\n",
        "## Como este colab funciona\n",
        "Ao executar as células fornecidas, trechos de código Python serão executados no runtime do Google. Uma VM é criada como ambiente de execução, e você tem recursos limitados. Estes recursos são resetados e zeram a cada 4 horas, então não faça treinos muito longos e lembre-se de salvar o seu progresso ao término de cada sessão de treino.\n",
        "\n",
        "Quando o tempo limite é atingido e o ambiente redefinido, é necessário remontar o seu drive e copiar novamente o checkpoint utilizado, além dos arquivos de treino. \n",
        "\n",
        "## Como usar este colab\n",
        "1. Faça uma cópia deste notebook para o seu drive\n",
        "2. Execute todo o processo no Chrome (ou navegadores baseados em Chromium)\n",
        "3. Execute as células providas abaixo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1HHp-FbilH4"
      },
      "source": [
        "# **Baixando o TensorFlow e do modelo**\n",
        "O download do TensorFlow deve ser executado a cada sessão de treino, pois entre o término de um processo de treino e o início do próximo é necessário reiniciar a VM. O download do modelo, no entando, deve ser executado uma vez somente, pois o download ficará no armazenamento virtual até que vençam as 4 horas de execução.\n",
        "\n",
        "Apenas os modelos 144M e o 355M são capazes de serem treinados, portanto prefira-os. O colab não tem recursos suficientes para treinar modelos maiores. Mas você pode baixá-los para usar no chatbot. As opções são\n",
        "\n",
        "* **124M:** o modelo \"small\", 500MB em disco.\n",
        "* **355M:** o modelo \"medium\", 1.5GB em disco.\n",
        "* **774M:** o modelo \"large\", não pode ser treinado no colab, mas pode ser usado no chatbot.\n",
        "* **1558M:** o modelo \"extra large\", não pode ser treinado no colab, mas pode ser usado no chatbot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTLh2PrrlZmW"
      },
      "source": [
        "# Definição de variáveis para execução (nomes padrão, podem ser alterados)\n",
        "gpt_model = \"355M\" # dar preferência para 355M ou 124M\n",
        "model_name = \"run1\" # nome do modelo gerado e treinado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ESGT-AxlEDo"
      },
      "source": [
        "# Iniciando o TensorFlow (deve ser executado uma vez por sessão)\n",
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        "import shutil\n",
        "import os\n",
        "import sys\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-DRC_jalP9F"
      },
      "source": [
        "# Download do modelo do GPT-2 (deve ser executado sempre que o ambiente for redefinido)\n",
        "gpt2.download_gpt2(model_name = gpt_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltGqjWkpnm7c"
      },
      "source": [
        "# **Utilizando GPU**\n",
        "Este colab utilizará GPU para treinamento do modelo, pois tem mais performance que CPU. O Colaboratory usa uma NVIDIA T4 ou uma K80 para executar procedimentos, dependendo da que estiver disponível. Execute o trecho de código abaixo para verificar qual GPU estará disponível para treino."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSqxDjqFn6T1"
      },
      "source": [
        "# Verificando GPU disponível\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doDi0UtmpJvW"
      },
      "source": [
        "# **Gerenciamento de armazenamento**\n",
        "É recomendável não subir arquivos diretamente para o ambiente de execução. Suba os arquivos para o seu drive, e copie-os de lá para cá. Assim, você mantém um backup deles para treinos posteriores. Além disso, você poderá também copiar seu modelo treinado para o drive, e depois copiá-lo de volta para cá e continuar o treino de onde parou.\n",
        "\n",
        "Seu modelo, ao ser copiado para o driver, será empacotado num arquivo `.tar`. Ao ser copiado de volta para o ambiente, o `.tar` será extraído e colocado na pasta correta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QKD37xgu6Vp",
        "outputId": "79dcf365-7777-4bbb-e4c8-b71cb4f8f8d5"
      },
      "source": [
        "# Montar o drive no ambiente\n",
        "gpt2.mount_gdrive()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bRhm56RqduO"
      },
      "source": [
        "# Copiar modelo daqui para o drive\n",
        "gpt2.copy_checkpoint_to_gdrive(run_name = model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuMfaeUIqf-S"
      },
      "source": [
        "# Copiar modelo do drive para cá\n",
        "gpt2.copy_checkpoint_from_gdrive(run_name = model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fR0d5wihqjeb"
      },
      "source": [
        "# Copiar arquivos do drive para cá\n",
        "dataset = \"highrock.txt\" # nome do arquivo de dados a ser copiado do drive\n",
        "gpt2.copy_file_from_gdrive(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCNczy3Im7H-"
      },
      "source": [
        "# **Treinamento do modelo**\n",
        "Com os diretórios criados, o TensorFlow iniciado e o modelo baixado, podemos dar início no treino do nosso modelo. O trecho de código abaixo inicia uma sessão do GPT-2. Esta sessão será usada para treino e para geração de texto a partir do modelo. \n",
        "\n",
        "Para treinar o modelo, você terá duas opções: uma inicia o treino do zero, e a segunda continua o treino de onde a sessão anterior parou. Lembre-se de copiar o modelo treinado entre o ambiente e o seu drive para não perdê-lo.\n",
        "\n",
        "Após a conclusão de uma sessão de treino, você poderá gerar quantas entradas de texto quiser, mas para treinar mais seu modelo, deverá reinciar a VM e iniciar uma nova sessão. \n",
        "\n",
        "Menu superior -> Runtime -> Restart runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMCQ1DjpnVke"
      },
      "source": [
        "# Iniciando uma sessão\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VNgcINrrgYV"
      },
      "source": [
        "# Iniciando um treino do zero\n",
        "gpt2.finetune(sess,\n",
        "              dataset = dataset,\n",
        "              model_name = gpt_model,\n",
        "              steps = 2000,\n",
        "              restore_from = 'fresh',\n",
        "              run_name = model_name,\n",
        "              print_every = 100,\n",
        "              sample_every = 500,\n",
        "              save_every = 500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kvZYkVRrj3q"
      },
      "source": [
        "# Continuando o treino\n",
        "gpt2.finetune(sess,\n",
        "              dataset = dataset,\n",
        "              model_name = gpt_model,\n",
        "              steps = 2000,\n",
        "              restore_from = 'latest',\n",
        "              run_name = model_name,\n",
        "              print_every = 100,\n",
        "              sample_every = 500,\n",
        "              save_every = 500,\n",
        "              overwrite = 'true')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AurViSTKsPZM"
      },
      "source": [
        "# **Geração de texto**\n",
        "Após o treinamento do seu modelo, será possível gerar entradas de texto com ele. Você pode gerar textos aleatórios ou a partir de um prompt, que será completado pelo modelo.\n",
        "\n",
        "Atente-se aos parâmetros passados na geração por prompt. Quanto menor a temperatura, mais conciso o texto. Por consequência, quanto maior a tempertura, mais maluco será o texto gerado. O valor padrão definido é `0.5`, mas é possível alterá-lo. O valor mínimo aceito pelo GPT-2 é `0.1`.\n",
        "\n",
        "* **length:** tamanho do texto gerado em caracteres\n",
        "* **temperature:** qualidade do texto gerado. Quanto maior o valor, pior a qualidade\n",
        "* **prefix:** valor de início, que a IA deverá completar com texto próprio\n",
        "* **nsamples:** número de textos gerados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Sfm0hSlsZnV"
      },
      "source": [
        "# Geração de texto aleatório\n",
        "gpt2.generate(sess, run_name = model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UzhH6XqsfJa"
      },
      "source": [
        "# Geração de texto a partir de prompt\n",
        "prompt = \"Balfiera is\"\n",
        "gpt2.generate(sess,\n",
        "              length = 200,\n",
        "              temperature = 0.5,\n",
        "              prefix = prompt,\n",
        "              nsamples = 15,\n",
        "              batch_size = 15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU0DbqIEI7bk"
      },
      "source": [
        "# **Chat com o modelo (em desenvolvimento)**\n",
        "Além de gerar textos, podemos usar o modelo de AI como um chatbot e interagir com ele.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJQQvqT3L8wr"
      },
      "source": [
        "# Declaração de varíaveis\n",
        "chatbot_model = 'run2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mMsOa4KCMHgE",
        "outputId": "f0d910e4-d9ab-43a8-83dd-6e8aad5ee5cb"
      },
      "source": [
        "# Cópia de modelo treinado para a pasta do chatbot (antes de usar o chatbot)\n",
        "shutil.move('checkpoint/' + model_name, 'models/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'models/run2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Bd98QvsHMoT6",
        "outputId": "b71a7145-0ed1-413d-a97b-5968ccf6a88e"
      },
      "source": [
        "# Cópia de modelo treinado de volta para a pasta original (depois de terminar de usar o chatbot)\n",
        "shutil.move('models/' + model_name, 'checkpoint/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'checkpoint/run2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ie-WZ_cBRMmj",
        "outputId": "d6541b34-319c-4e7e-9757-a6bbba2b21eb"
      },
      "source": [
        "# Clone do projeto do GPT\n",
        "!git clone https://github.com/nshepperd/gpt-2.git\n",
        "\n",
        "# Importação das libs\n",
        "sys.path.append(\"/content/gpt-2/src\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'gpt-2' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfoIdz1xQM2j"
      },
      "source": [
        "# Criação do chatbot\n",
        "\n",
        "!pip install fire\n",
        "\n",
        "import fire\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import model, sample, encoder\n",
        "import generate_unconditional_samples\n",
        "import interactive_conditional_samples\n",
        "\n",
        "class GPT2:\n",
        "\n",
        "  \n",
        "  # extracted from the source code to generate some text based on a prior\n",
        "  def __init__(\n",
        "      self,\n",
        "      model_name=chatbot_model,\n",
        "      seed=None,\n",
        "      nsamples=1,\n",
        "      batch_size=1,\n",
        "      length=None,\n",
        "      temperature=1,\n",
        "      top_k=0,\n",
        "      raw_text=\"\",\n",
        "  ):\n",
        "      \"\"\"\n",
        "      Interactively run the model\n",
        "      :model_name=117M : String, which model to use\n",
        "      :seed=None : Integer seed for random number generators, fix seed to reproduce\n",
        "       results\n",
        "      :nsamples=1 : Number of samples to return total\n",
        "      :batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples.\n",
        "      :length=None : Number of tokens in generated text, if None (default), is\n",
        "       determined by model hyperparameters\n",
        "      :temperature=1 : Float value controlling randomness in boltzmann\n",
        "       distribution. Lower temperature results in less random completions. As the\n",
        "       temperature approaches zero, the model will become deterministic and\n",
        "       repetitive. Higher temperature results in more random completions.\n",
        "      :top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
        "       considered for each step (token), resulting in deterministic completions,\n",
        "       while 40 means 40 words are considered at each step. 0 (default) is a\n",
        "       special setting meaning no restrictions. 40 generally is a good value.\n",
        "      \"\"\"\n",
        "      if batch_size is None:\n",
        "          batch_size = 1\n",
        "      assert nsamples % batch_size == 0\n",
        "\n",
        "      self.nsamples = nsamples\n",
        "      self.batch_size = batch_size\n",
        "      \n",
        "      self.enc = encoder.get_encoder(model_name)\n",
        "      hparams = model.default_hparams()\n",
        "      with open(os.path.join('models', model_name, 'hparams.json')) as f:\n",
        "          hparams.override_from_dict(json.load(f))\n",
        "\n",
        "      if length is None:\n",
        "          length = hparams.n_ctx // 2\n",
        "      elif length > hparams.n_ctx:\n",
        "          raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "      self.sess = tf.Session(graph=tf.Graph())\n",
        "      self.sess.__enter__()\n",
        "      \n",
        "      self.context = tf.placeholder(tf.int32, [batch_size, None])\n",
        "      np.random.seed(seed)\n",
        "      tf.set_random_seed(seed)\n",
        "      self.output = sample.sample_sequence(\n",
        "          hparams=hparams, length=length,\n",
        "          context=self.context,\n",
        "          batch_size=batch_size,\n",
        "          temperature=temperature, top_k=top_k\n",
        "      )\n",
        "\n",
        "      saver = tf.train.Saver()\n",
        "      self.ckpt = tf.train.latest_checkpoint(os.path.join('models', model_name))\n",
        "      saver.restore(self.sess, self.ckpt)\n",
        "\n",
        "  def close(self):\n",
        "    self.sess.close()\n",
        "  \n",
        "  def generate_conditional(self,raw_text):\n",
        "      context_tokens = self.enc.encode(raw_text)\n",
        "      generated = 0\n",
        "      for _ in range(self.nsamples // self.batch_size):\n",
        "          out = self.sess.run(self.output, feed_dict={\n",
        "              self.context: [context_tokens for _ in range(self.batch_size)]\n",
        "          })[:, len(context_tokens):]\n",
        "          for i in range(self.batch_size):\n",
        "              generated += 1\n",
        "              text = self.enc.decode(out[i])\n",
        "              return text\n",
        "              #print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
        "              #print(text)\n",
        "      #print(\"=\" * 80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkFZOFgjQRjZ"
      },
      "source": [
        "# Carregar o modelo\n",
        "gpt2 = GPT2(model_name=chatbot_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_4moc5XQTSh"
      },
      "source": [
        "result = gpt2.generate_conditional(raw_text=\"How are you?\")\n",
        "\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc2MuW8stdrD"
      },
      "source": [
        "# **Troubleshooting**\n",
        "No caso de problemas, mate o ambiente e recrie-o. O comando abaixo faz isso. Lembre-se: ao redefinir o ambiente, será necessário baixar o modelo, iniciar o TensorFlow e baixar todos os seus arquivos de volta do drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3agbcKDtspD"
      },
      "source": [
        "# Reiniciar o ambiente\n",
        "!kill -9 -1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}